<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>FlowGRU</title>
	<meta name="author" content="SLV-lab">

	<link href="./css/bootstrap.min.css" rel="stylesheet">
  <link href="./css/style.css" rel="stylesheet">

</head>

  <body>

    <div class="container">
      <div class="header">
        <h3> <center>Temporally Consistent Depth Prediction with Flow-Guided Memory Units</center> </h3>
      </div>
      
      <h3><center>
      <iframe width="100%" height="512" src="https://www.youtube.com/embed/wCHXZrnxLws" allowfullscreen>></iframe>
      </center></h3>

      <div class="row">
      	<h3>Authors</h3>
      	<div style="font-size: 16px">
      	<ul>
            <li><a href="https://chanhoeom.github.io/">Chanho Eom</a></li>
            <li><a href="https://hyunjp.github.io/">Hyunjong Park</a></li>
            <li><a href="https://bsham.github.io/">Bumsub Ham</a></li>
      	</ul>

      </div>

      <div class="row">
        <h3>Abstract</h3>
        <p style="text-align: justify;">
        Predicting depth from a monocular video sequence has advanced considerably in the past few years. Recent methods based on convolutional neural networks (CNNs), however, discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes spatial and temporal features, respectively. The memory module inputs spatial and temporal features sequentially together with optical flow tailored to our task. We implement the memory module with convolutional gated recurrent units (ConvGRUs). It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction. 
        </p>
      </div>

      <div class="row">
      	<h3>Framework</h3>
      	<center>
        <img src="./images/timestep.png" style="max-width:90%; margin-left: 28px; padding-top: 25px; padding-bottom: 25px;">
      	</center>
      </div>

      <div class="row">
        <h3>Flow-Guided Memory</h3>
        We propose a novel memory unit, dubbed FlowGRU, that stores trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results.

        <center>
        <img src="./images/flowgru.png" style="max-width:30%; margin-left: 28px; padding-top: 25px; padding-bottom: 25px;">
        </center>
      </div>

      <div class="row">
        <h3>Paper</h3>
		<p>
     	</p><table>
  		<tbody><tr></tr>
  		<tr><td>
    	<img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="./images/paper.png" width="150px">
  		</td>
 		<td></td>
  		<td>
    	C. Eom, H. Park, B. Ham<br>
    	<b>Temporally Consistent Depth Prediction with Flow-Guided Memory Units</b> <br>
      [<a href="https://ieeexplore.ieee.org/document/8848860">Paper</a>] [<a href="https://github.com/cvlab-yonsei/FlowGRU">Code</a>]
		</td></tr></tbody></table>
     
     		<pre><tt>@InProceedings{eom2019temporally,
	        author       = "C. Eom, H. Park, B. Ham",
	        title        = "Temporally Consistent Depth Prediction with Flow-Guided Memory Units",
	        booktitle    = "IEEE transactions on intelligent transportation systems",
	        year         = "2019",
	        }</tt></pre>

      </div>
      
      <div class="row">
        <h3>Acknowledgements</h3>
        <p>
        This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.2016-0- 00197, Development of the high-precision natural 3D view generation technology using smart-car multi sensors and deep learning).
		</p>
      </div>

    </div>
