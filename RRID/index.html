<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>RRID</title>
	<meta name="author" content="SLV-lab">

	<link href="./css/bootstrap.min.css" rel="stylesheet">
    <link href="./css/style.css" rel="stylesheet">

</head>

  <body>

    <div class="container">
      <div class="header">
        <h3> <center> Relation Network for Person Re-identification </center> </h3>
        <h4 style="color: #517CB9; font-size: 30px"> <center> <a href="https://aaai.org/Conferences/AAAI-20/"><b>*AAAI-2020*</b></a> </center></center></h4>
      </div>

<!--
      <center>
        <img src="./SFNet_files/teaser.png" style="max-width:100%;">
      </center>
-->

      <div class="row">
      	<h3>Authors</h3>
      	<div style="font-size: 16px">
      	<ul>
            <li><a href="https://github.com/hyunjp">Hyunjong Park</a></li>
            <li><a href="https://bsham.github.io/">Bumsub Ham</a></li>
      	</ul>
      	</div>    

      </div>

      <div class="row">
        <h3>Abstract</h3>
        <p style="text-align: justify;">
        Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.  
        </p>
      </div>


      <div class="row">
      	<h3>Overview of our architecture</h3>
      	<center>
        <img src="./RRID_files/Overview.png" style="max-width:90%;">
      	</center>
      </div>
<!--
      <div class="row">
        <h3>Paper</h3>
	<p>
     </p><table>
  <tbody><tr></tr>
  <tr><td>
    <a href="https://arxiv.org/abs/1904.01810"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="./SFNet_files/thb.png" width="150px"></a>
  </td>
  <td></td>
  <td>
    J. Lee, D. Kim, J. Ponce, B. Ham<br>
    <b>SFNet: Learning Object-aware Semantic Flow</b> <br>
    In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019 (Oral) <br>
    [<a href="https://arxiv.org/abs/1904.01810">Paper on arXiv</a>]
</td></tr></tbody></table>
     
      <h3>BibTeX</h3>
     <pre><tt>@InProceedings{Lee19,
        author       = "J. Lee, D. Kim, J. Ponce, B. Ham",
        title        = "SFNet: Learning Object-aware Semantic Flow",
        booktitle    = "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
        year         = "2019",
        }</tt></pre>

      </div>
      <div class="row">

        <h3>Code</h3>
        <p> <a href="https://github.com/cvlab-yonsei/projects/tree/master/SFNet/codes"> Training/testing code (PyTorch 1.0) </a>
          </p>
      </div>

-->
      
      
      <div class="row">
        <h3>Acknowledgements</h3>
        <p>
        This research was supported by R&D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA (NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).
        
		</p>
      </div>

    </div>
