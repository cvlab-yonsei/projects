{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bham/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/bham/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import png\n",
    "\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "import dataset_main_test\n",
    "from read_depth import depth_read\n",
    "from python_flo import readFLO\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test fol :  27\n"
     ]
    }
   ],
   "source": [
    "ds_class = dataset_main_test.Dataset( )\n",
    "\n",
    "test_path = '../../folder/eigen_test.txt'\n",
    "\n",
    "_, _, test_list = ds_class.get_list( test_list_path=test_path )\n",
    "\n",
    "print( 'Total test fol : ', len(test_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KITTI_path = os.path.abspath('/media/chanho/datasets/KITTI_raw_data/')\n",
    "KITTI_path = os.path.abspath('/mnt/sdb/datasets/KITTI_raw_data/')\n",
    "INTP_path  = os.path.join( KITTI_path, 'depth_interpolation' )\n",
    "GT_path = os.path.join(KITTI_path, 'eigen_test_gt_from_lidar')\n",
    "RGB_path = os.path.join(KITTI_path, 'RGB')\n",
    "OPT_path = os.path.join(KITTI_path, 'optflw_dis_inv')\n",
    "# OPT_path = os.path.join('/mnt/sdd/PWC_flow')\n",
    "#PRED_path  = os.path.abspath('/mnt/sdd/test_eigen/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(pred_path, gt_path):\n",
    "    \n",
    "    pred = depth_read(pred_path)\n",
    "    gt   = depth_read(gt_path)\n",
    "    \n",
    "    mask = (gt != -1)\n",
    "    gt[np.invert(mask)] = np.nan\n",
    "\n",
    "    gt_median   = np.nanmedian(gt)\n",
    "    pred_median = np.nanmedian(pred)\n",
    "    \n",
    "    pred *= gt_median / pred_median\n",
    "\n",
    "    return pred\n",
    "    \n",
    "def bound(pred, min_dep=1e-3, max_dep=50):\n",
    "    \n",
    "    pred[pred<min_dep] = min_dep\n",
    "    pred[pred>max_dep] = max_dep\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx(height, width):\n",
    "\n",
    "    idx_x = [ [ i for i in range(width)] for j in range(height) ]\n",
    "    idx_y = []\n",
    "\n",
    "    for j in range(height):\n",
    "        idx_y.append([ j for i in range(width) ])\n",
    "\n",
    "    idx = np.zeros((height, width, 2))\n",
    "    idx[:, :, 0] = idx_x\n",
    "    idx[:, :, 1] = idx_y\n",
    "\n",
    "    idx_tensor = tf.constant(idx, dtype=tf.float32)\n",
    "\n",
    "    return idx_tensor\n",
    "\n",
    "def warp(pst, back_flw):\n",
    "\n",
    "    h, w, _ = back_flw.get_shape().as_list()\n",
    "    index = idx(h, w)\n",
    "\n",
    "    a_1 = tf.zeros_like(pst)\n",
    "    a_2 = tf.zeros_like(pst)\n",
    "    a_3 = tf.zeros_like(pst)\n",
    "    a_4 = tf.zeros_like(pst)\n",
    "\n",
    "    x_n = tf.clip_by_value(index[:,:,0] + back_flw[:,:,0], 0, w-1)\n",
    "    y_n = tf.clip_by_value(index[:,:,1] + back_flw[:,:,1], 0, h-1)\n",
    "\n",
    "    x = tf.clip_by_value(tf.floor(index[:,:,0] + back_flw[:,:,0]), 0, w-1)\n",
    "    y = tf.clip_by_value(tf.floor(index[:,:,1] + back_flw[:,:,1]), 0, h-1)\n",
    "\n",
    "    x_1 = tf.clip_by_value(x+1, 0, w-1)\n",
    "    y_1 = tf.clip_by_value(y+1, 0, h-1)\n",
    "\n",
    "    a_1 = tf.multiply(tf.expand_dims(tf.multiply(1.0-tf.abs(x_n - x), 1.0-tf.abs(y_n - y)), axis=2),\n",
    "                      tf.gather_nd(pst, tf.to_int32(tf.stack([y,x],axis=2))))\n",
    "    a_2 = tf.multiply(tf.expand_dims(tf.multiply(1.0-tf.abs(x_n - (x+1)), 1.0-tf.abs(y_n - y)), axis=2),\n",
    "                      tf.gather_nd(pst, tf.to_int32(tf.stack([y,x_1],axis=2))))\n",
    "    a_3 = tf.multiply(tf.expand_dims(tf.multiply(1.0-tf.abs(x_n - x), 1.0-tf.abs(y_n - (y+1))), axis=2),\n",
    "                      tf.gather_nd(pst, tf.to_int32(tf.stack([y_1,x],axis=2))))\n",
    "    a_4 = tf.multiply(tf.expand_dims(tf.multiply(1.0-tf.abs(x_n - (x+1)), 1.0-tf.abs(y_n - (y+1))), axis=2),\n",
    "                      tf.gather_nd(pst, tf.to_int32(tf.stack([y_1,x_1],axis=2))))\n",
    "\n",
    "    return a_1 + a_2 + a_3 + a_4\n",
    "\n",
    "def pred_warp(prev_pred, backflw):\n",
    "    \n",
    "    return tf.map_fn(lambda inputs : warp(inputs[0],inputs[1]), elems=[prev_pred, backflw], dtype=tf.float32)\n",
    "\n",
    "def no_mask(appr_X, p_appr_X, backflw, alpha=0.5):\n",
    "\n",
    "    wrpd_p_appr_X = tf.map_fn(lambda inputs : warp(inputs[0],inputs[1]), elems=[p_appr_X, backflw], dtype=tf.float32)\n",
    "\n",
    "    diff = tf.reduce_mean(tf.abs(appr_X - wrpd_p_appr_X), 3) \n",
    "    msk  = tf.exp((-1) * alpha * diff)\n",
    "\n",
    "    return msk\n",
    "    \n",
    "def readFLO(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic = np.fromfile(f, np.float32, count=1)\n",
    "        \n",
    "        if 202021.25 != magic:\n",
    "            print('Magic number incorrect. Invalid .flo file')\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            w = np.fromfile(f, np.int32, count=1)[0]\n",
    "            h = np.fromfile(f, np.int32, count=1)[0]\n",
    "            #print('Reading %d x %d flo file' % (h, w))\n",
    "            data = np.fromfile(f, np.float32, count=2*w*h)\n",
    "            # Reshape data into 3D array (columns, rows, bands)\n",
    "            data2D = np.resize(data, (h, w, 2))\n",
    "            return data2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order(PRED_path, SAVE_path, ours=False):\n",
    "\n",
    "    total_error = 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    height = 376\n",
    "    width  = 1242\n",
    "    \n",
    "    p_appr_X = tf.placeholder(tf.float32, [1, height, width, 3])\n",
    "    appr_X   = tf.placeholder(tf.float32, [1, height, width, 3])\n",
    "    tmpr_X   = tf.placeholder(tf.float32, [1, height, width, 2])\n",
    "    p_dep    = tf.placeholder(tf.float32, [1, height, width])\n",
    "\n",
    "    mask = tf.squeeze(no_mask(appr_X, p_appr_X, tmpr_X, alpha=0.5)) > 0.05\n",
    "    wrpd_prev_pred = tf.squeeze(pred_warp(tf.expand_dims(p_dep, 3), tmpr_X))\n",
    "\n",
    "    for fol in range(len(test_list)):\n",
    "    #for fol in range(1):\n",
    "\n",
    "        video_split = len(next(os.walk('/mnt/sdb/datasets/KITTI_raw_data/RGB/%s%s/' % (test_list[fol], 'image_02/data')))[2])\n",
    "\n",
    "        for frame_num in range(2, video_split-1):\n",
    "        #for frame_num in range(2, 3):\n",
    "            \n",
    "            #< ----------------------- Data Path ----------------------- >#\n",
    "            full = test_list[fol].split()[0]\n",
    "            split = test_list[fol].split()[0].split('/')\n",
    "\n",
    "            if ours:\n",
    "                \n",
    "                prev_pred_path = os.path.join(PRED_path, full, '%010d.png'%(frame_num-1))\n",
    "                pred_path      = os.path.join(PRED_path, full, '%010d.png'%frame_num)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                prev_pred_path = os.path.join(PRED_path, full, 'image_02/data', '%010d.png'%(frame_num-1))        \n",
    "                pred_path      = os.path.join(PRED_path, full, 'image_02/data', '%010d.png'%frame_num)\n",
    "                \n",
    "            prev_gt_path  = os.path.join(GT_path, full, 'image_02', '%010d.png'%(frame_num-1))\n",
    "            gt_path       = os.path.join(GT_path, full, 'image_02', '%010d.png'%frame_num)\n",
    "            prev_rgb_path = os.path.join(RGB_path, full, 'image_02', 'data', '%010d.png'%(frame_num-1))\n",
    "            rgb_path      = os.path.join(RGB_path, full, 'image_02', 'data', '%010d.png'%frame_num)\n",
    "            flw_path      = os.path.join(OPT_path, full, 'image_02', 'data', '%010d.flo'%frame_num)\n",
    "\n",
    "            #< ----------------------- Data Load  ----------------------- >#\n",
    "\n",
    "            prev_pred = scale(prev_pred_path, prev_gt_path)\n",
    "            pred = scale(pred_path, gt_path)\n",
    "            prev_rgb = scipy.misc.imread(prev_rgb_path)\n",
    "            rgb = scipy.misc.imread(rgb_path)\n",
    "            flw = readFLO(flw_path)\n",
    "\n",
    "            #rgb_h, rgb_w, _  = np.shape(rgb)\n",
    "            prev_pred = cv2.resize(np.squeeze(prev_pred), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            pred = cv2.resize(np.squeeze(pred), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            prev_rgb = cv2.resize(np.squeeze(prev_rgb), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            rgb = cv2.resize(np.squeeze(rgb), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            flw = cv2.resize(np.squeeze(flw), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            M = mask.eval(feed_dict={p_appr_X: [prev_rgb], appr_X: [rgb], tmpr_X: [flw]})\n",
    "            Wrpd = wrpd_prev_pred.eval(feed_dict={p_dep: [prev_pred], tmpr_X: [flw]})\n",
    "\n",
    "            pred_diff_1 = M * np.abs(Wrpd - pred)\n",
    "            masked_mean_error = np.sum(pred_diff_1) / np.sum(M)\n",
    "\n",
    "            total_error += masked_mean_error\n",
    "            cnt += 1\n",
    "\n",
    "            if not os.path.exists(os.path.join(SAVE_path, split[0])):\n",
    "                os.makedirs(os.path.join(SAVE_path, split[0]))\n",
    "            if not os.path.exists(os.path.join(SAVE_path, split[0], split[1])):\n",
    "                os.makedirs(os.path.join(SAVE_path, split[0], split[1]))\n",
    "\n",
    "            with open(os.path.join(SAVE_path, split[0], split[1], 'log.txt'), 'a') as f:\n",
    "                f.write('%.5f\\n'%(masked_mean_error))\n",
    "                \n",
    "    return total_error / cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_first_order(PRED_path, SAVE_path):\n",
    "\n",
    "    total_error = 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    height = 376\n",
    "    width  = 1242\n",
    "    \n",
    "    p_appr_X = tf.placeholder(tf.float32, [1, height, width, 3])\n",
    "    appr_X   = tf.placeholder(tf.float32, [1, height, width, 3])\n",
    "    tmpr_X   = tf.placeholder(tf.float32, [1, height, width, 2])\n",
    "    p_dep    = tf.placeholder(tf.float32, [1, height, width])\n",
    "\n",
    "    mask = tf.squeeze(no_mask(appr_X, p_appr_X, tmpr_X, alpha=0.5)) > 0.05\n",
    "    wrpd_prev_pred = tf.squeeze(pred_warp(tf.expand_dims(p_dep, 3), tmpr_X))\n",
    "\n",
    "    for fol in range(len(test_list)):\n",
    "    #for fol in range(1):\n",
    "\n",
    "        video_split = len(next(os.walk('/mnt/sdb/datasets/KITTI_raw_data/RGB/%s%s/' % (test_list[fol], 'image_02/data')))[2])\n",
    "\n",
    "        for frame_num in range(2, video_split-1):\n",
    "        #for frame_num in range(2, 3):\n",
    "            \n",
    "            #< ----------------------- Data Path ----------------------- >#\n",
    "            full = test_list[fol].split()[0]\n",
    "            split = test_list[fol].split()[0].split('/')\n",
    "                \n",
    "            prev_pred_path = os.path.join(PRED_path, full, 'proj_depth/groundtruth/image_02', '%010d.png'%(frame_num-1))        \n",
    "            pred_path      = os.path.join(PRED_path, full, 'proj_depth/groundtruth/image_02', '%010d.png'%frame_num)\n",
    "            prev_rgb_path = os.path.join(RGB_path, full, 'image_02', 'data', '%010d.png'%(frame_num-1))\n",
    "            rgb_path      = os.path.join(RGB_path, full, 'image_02', 'data', '%010d.png'%frame_num)\n",
    "            flw_path      = os.path.join(OPT_path, full, 'image_02', 'data', '%010d.flo'%frame_num)\n",
    "\n",
    "            #< ----------------------- Data Load  ----------------------- >#\n",
    "\n",
    "            prev_pred = depth_read(prev_pred_path)\n",
    "            pred = depth_read(pred_path)\n",
    "            prev_rgb = scipy.misc.imread(prev_rgb_path)\n",
    "            rgb = scipy.misc.imread(rgb_path)\n",
    "            flw = readFLO(flw_path)\n",
    "\n",
    "            #rgb_h, rgb_w, _  = np.shape(rgb)\n",
    "            prev_pred = cv2.resize(np.squeeze(prev_pred), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            pred = cv2.resize(np.squeeze(pred), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            prev_rgb = cv2.resize(np.squeeze(prev_rgb), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            rgb = cv2.resize(np.squeeze(rgb), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            flw = cv2.resize(np.squeeze(flw), (width, height), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            garg_crop = np.array([0.5*height, 0.99189189*height, 0.03594771*width, 0.96405229*width]).astype(np.int32)\n",
    "            crop_mask = np.zeros((height, width))\n",
    "            crop_mask[garg_crop[0]:garg_crop[1],garg_crop[2]:garg_crop[3]] = 1\n",
    "            M = mask.eval(feed_dict={p_appr_X: [prev_rgb], appr_X: [rgb], tmpr_X: [flw]})\n",
    "\n",
    "            MSK  = np.logical_and(crop_mask, M)\n",
    "            Wrpd = wrpd_prev_pred.eval(feed_dict={p_dep: [prev_pred], tmpr_X: [flw]})\n",
    "\n",
    "            pred_diff_1 = MSK * np.abs(Wrpd - pred)\n",
    "            masked_mean_error = np.sum(pred_diff_1) / np.sum(MSK)\n",
    "\n",
    "            total_error += masked_mean_error\n",
    "            cnt += 1\n",
    "\n",
    "            if not os.path.exists(os.path.join(SAVE_path, split[0])):\n",
    "                os.makedirs(os.path.join(SAVE_path, split[0]))\n",
    "            if not os.path.exists(os.path.join(SAVE_path, split[0], split[1])):\n",
    "                os.makedirs(os.path.join(SAVE_path, split[0], split[1]))\n",
    "\n",
    "            with open(os.path.join(SAVE_path, split[0], split[1], 'log.txt'), 'a') as f:\n",
    "                f.write('%.5f\\n'%(masked_mean_error))\n",
    "                \n",
    "    return total_error / cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './temp/final1_DIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_path = os.path.abspath(save_path + 'GT')\n",
    "error1 = gt_first_order(INTP_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('GT', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/SfMLearner-master_Zhou/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Zhou')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Zhou', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/LKVOLearner-master_Wang/src/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Wang')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Wang', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/GeoNet-master/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'GeoNet')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('GeoNet', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/home/bham/HyunChan/Godard-monodepth-master/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Godard')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Godard', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/Garg/depth_80/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Garg')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Garg', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/semodepth-master_Kuznietsov/inference/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Kuznietsov')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Kuznietsov', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/flowgru_other_methods/DORN-master_Fu/depth/')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'Fu')\n",
    "error1 = first_order(PRED_path, SAVE_path)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('Fu', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/main_lrsch_smth_multiGPU_193')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'main_lrsch_smth_multiGPU_193')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('main_lrsch_smth_multiGPU_193', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/base_flowgru_multiGPU2_180')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'base_flowgru_multiGPU_180')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('base_flowgru_multiGPU_180', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/base_flowgru_multiGPU2_CS_KITTI_epoch085')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'base_flowgru_multiGPU_CS_KITTI_epoch085')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)\n",
    "\n",
    "with open(os.path.join(save_path, 'total.txt'), 'a') as f:\n",
    "    f.write('%30s %.5f\\n'%('base_flowgru_multiGPU_CS_KITTI_epoch085', error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.042013934292797\n"
     ]
    }
   ],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/main_lrsch_smth_multiGPU_100')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'main_lrsch_smth_multiGPU_100')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0273154337679231\n"
     ]
    }
   ],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/base_flowgru_multiGPU2_wofrn_096')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'base_flowgru_multiGPU2_wofrn_096')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0090761705663338\n"
     ]
    }
   ],
   "source": [
    "PRED_path = os.path.abspath('/mnt/sdd/test_eigen/base_flowgru_multiGPU2_100')\n",
    "\n",
    "SAVE_path = os.path.abspath(save_path + 'base_flowgru_multiGPU_100')\n",
    "error1 = first_order(PRED_path, SAVE_path, True)\n",
    "print(error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
